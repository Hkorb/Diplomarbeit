
@misc{noauthor_machine_nodate,
	title = {Machine learning and the physical sciences},
	url = {https://www.researchgate.net/publication/332010806_Machine_learning_and_the_physical_sciences},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2020-02-06},
	journal = {ResearchGate},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/IWFAZPK5/332010806_Machine_learning_and_the_physical_sciences.html:text/html;Volltext:/home/henry/snap/zotero-snap/10/Zotero/storage/SJBP7WKK/(PDF) Machine learning and the physical sciences.pdf:application/pdf}
}

@book{duriez_machine_2017,
	title = {Machine {Learning} {Control}-{Taming} {Nonlinear} {Dynamics} and {Turbulence}},
	publisher = {Springer},
	author = {Duriez, Thomas and Brunton, Steven L and Noack, Bernd R},
	year = {2017},
	file = {Duriez et al. - 2017 - Machine Learning Control-Taming Nonlinear Dynamics.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/7RF2NYGT/Duriez et al. - 2017 - Machine Learning Control-Taming Nonlinear Dynamics.pdf:application/pdf}
}

@article{rabault_artificial_2018,
	title = {Artificial {Neural} {Networks} trained through {Deep} {Reinforcement} {Learning} discover control strategies for active flow control},
	doi = {10.1017/jfm.2019.62},
	abstract = {We present the first application of an Artificial Neural Network trained through a Deep Reinforcement Learning agent to perform active flow control. It is shown that, in a 2D simulation of the Karman vortex street at moderate Reynolds number (Re = 100), our Artificial Neural Network is able to learn an active control strategy from experimenting with the mass flow rates of two jets on the sides of a cylinder. By interacting with the unsteady wake, the Artificial Neural Network successfully stabilizes the vortex alley and reduces drag by about 8\%. This is performed while using small mass flow rates for the actuation, on the order of 0.5\% of the mass flow rate intersecting the cylinder cross section once a new pseudo-periodic shedding regime is found. This opens the way to a new class of methods for performing active flow control.},
	author = {Rabault, Jean and Kuchta, Miroslav and Jensen, Atle and Reglade, Ulysse and Cerardi, Nicolas},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07664v5},
	keywords = {physics.flu-dyn}
}

@article{rabault_accelerating_2019,
	title = {Accelerating {Deep} {Reinforcement} {Learning} strategies of {Flow} {Control} through a multi-environment approach},
	doi = {10.1063/1.5116415},
	abstract = {Deep Reinforcement Learning (DRL) has recently been proposed as a methodology to discover complex Active Flow Control (AFC) strategies [Rabault, J., Kuchta, M., Jensen, A., Reglade, U., \& Cerardi, N. (2019): "Artificial neural networks trained through deep reinforcement learning discover control strategies for active flow control", Journal of Fluid Mechanics, 865, 281-302]. However, while promising results were obtained on a simple 2D benchmark flow at a moderate Reynolds number, considerable speedups will be required to investigate more challenging flow configurations. In the case of DRL trained with Computational Fluid Dynamics (CFD) data, it was found that the CFD part, rather than training the Artificial Neural Network, was the limiting factor for speed of execution. Therefore, speedups should be obtained through a combination of two approaches. The first one, which is well documented in the literature, is to parallelize the numerical simulation itself. The second one is to adapt the DRL algorithm for parallelization. Here, a simple strategy is to use several independent simulations running in parallel to collect experiences faster. In the present work, we discuss this solution for parallelization. We illustrate that perfect speedups can be obtained up to the batch size of the DRL agent, and slightly suboptimal scaling still takes place for an even larger number of simulations. This is, therefore, an important step towards enabling the study of more sophisticated Fluid Mechanics problems through DRL.},
	author = {Rabault, Jean and Kuhnle, Alexander},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.10382v3},
	keywords = {physics.comp-ph}
}

@article{rabault_deep_2018,
	title = {Deep {Reinforcement} {Learning} achieves flow control of the {2D} {Karman} {Vortex} {Street}},
	abstract = {The Karman Vortex Street has been investigated for over a century and offers a reference case for investigation of flow stability and control of high dimensionality, non-linear systems. Active flow control, while of considerable interest from a theoretical point of view and for industrial applications, has remained inaccessible due to the difficulty in finding successful control strategies. Here we show that Deep Reinforcement Learning can achieve a stable active control of the Karman vortex street behind a two-dimensional cylinder. Our results show that Deep Reinforcement Learning can be used to design active flow controls and is a promising tool to study high dimensionality, non-linear, time dependent dynamic systems present in a wide range of scientific problems.},
	author = {Rabault, Jean and Reglade, Ulysse and Cerardi, Nicolas and Kuchta, Miroslav and Jensen, Atle},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.10754v1},
	keywords = {physics.flu-dyn}
}

@article{rohrig_powering_2019,
	title = {Powering the 21st century by wind energy—{Options}, facts, figures},
	volume = {6},
	doi = {10.1063/1.5089877},
	number = {3},
	journal = {Applied Physics Reviews},
	author = {Rohrig, K. and Berkhout, V. and Callies, D. and Durstewitz, M. and Faulstich, S. and Hahn, B. and Jung, M. and Pauscher, L. and Seibel, A. and Shan, M. and Siefert, M. and Steffen, J. and Collmann, M. and Czichon, S. and Dörenkämper, M. and Gottschall, J. and Lange, B. and Ruhle, A. and Sayer, F. and Stoevesandt, B. and Wenske, J.},
	month = sep,
	year = {2019},
	pages = {031303},
	file = {Rohrig et al. - 2019 - Powering the 21st century by wind energy—Options, .pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/DER9ZA7Y/Rohrig et al. - 2019 - Powering the 21st century by wind energy—Options, .pdf:application/pdf}
}

@misc{noauthor_comprehensive_nodate,
	title = {Comprehensive comparison of collision models in the lattice {Boltzmann} framework: {Theoretical} investigations},
	shorttitle = {({PDF}) {Comprehensive} comparison of collision models in the lattice {Boltzmann} framework},
	url = {https://www.researchgate.net/publication/328734331_Comprehensive_comparison_of_collision_models_in_the_lattice_Boltzmann_framework_Theoretical_investigations},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2020-02-10},
	journal = {ResearchGate},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/H8VR7CQX/328734331_Comprehensive_comparison_of_collision_models_in_the_lattice_Boltzmann_framework_Theor.html:text/html}
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the beneﬁts of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	language = {en},
	urldate = {2020-02-12},
	journal = {arXiv:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = aug,
	year = {2017},
	note = {arXiv: 1707.06347},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/DV5FYAJZ/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf}
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2020-02-13},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/EVWY8GYP/Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf}
}

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, tile new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference method{\textasciitilde} have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, telnporal-differenee methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporaldifference methods can be applied to advantage.},
	language = {en},
	number = {1},
	urldate = {2020-02-14},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	pages = {9--44},
	file = {Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/IEP55E6I/Sutton - 1988 - Learning to predict by the methods of temporal dif.pdf:application/pdf}
}

@article{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difﬁculty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the ﬁrst challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(λ). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
	language = {en},
	urldate = {2020-02-14},
	journal = {arXiv:1506.02438 [cs]},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv: 1506.02438},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generali.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/UFCUZB7G/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generali.pdf:application/pdf}
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Sutton und Barto - 2018 - Reinforcement learning an introduction.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/6GBMPJIS/Sutton und Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf}
}

@article{pham_autonomous_2018,
	title = {Autonomous {UAV} {Navigation} {Using} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1801.05086},
	abstract = {Unmanned aerial vehicles (UAV) are commonly used for missions in unknown environments, where an exact mathematical model of the environment may not be available. This paper provides a framework for using reinforcement learning to allow the UAV to navigate successfully in such environments. We conducted our simulation and real implementation to show how the UAVs can successfully learn to navigate through an unknown environment. Technical aspects regarding to applying reinforcement learning algorithm to a UAV system and UAV ﬂight control were also addressed. This will enable continuing research using a UAV with learning capabilities in more important applications, such as wildﬁre monitoring, or search and rescue missions.},
	language = {en},
	urldate = {2020-02-20},
	journal = {arXiv:1801.05086 [cs]},
	author = {Pham, Huy X. and La, Hung M. and Feil-Seifer, David and Nguyen, Luan V.},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05086},
	keywords = {Computer Science - Robotics},
	file = {Pham et al. - 2018 - Autonomous UAV Navigation Using Reinforcement Lear.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/KFWEUD73/Pham et al. - 2018 - Autonomous UAV Navigation Using Reinforcement Lear.pdf:application/pdf}
}

@article{koch_reinforcement_2018,
	title = {Reinforcement {Learning} for {UAV} {Attitude} {Control}},
	url = {http://arxiv.org/abs/1804.04154},
	abstract = {Autopilot systems are typically composed of an “inner loop” providing stability and control, while an “outer loop” is responsible for mission-level objectives, e.g. way-point navigation. Autopilot systems for UAVs are predominately implemented using Proportional, Integral Derivative (PID) control systems, which have demonstrated exceptional performance in stable environments. However more sophisticated control is required to operate in unpredictable, and harsh environments. Intelligent ﬂight control systems is an active area of research addressing limitations of PID control most recently through the use of reinforcement learning (RL) which has had success in other applications such as robotics. However previous work has focused primarily on using RL at the mission-level controller. In this work, we investigate the performance and accuracy of the inner control loop providing attitude control when using intelligent ﬂight control systems trained with the state-of-theart RL algorithms, Deep Deterministic Gradient Policy (DDGP), Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO). To investigate these unknowns we ﬁrst developed an open-source high-ﬁdelity simulation environment to train a ﬂight controller attitude control of a quadrotor through RL. We then use our environment to compare their performance to that of a PID controller to identify if using RL is appropriate in high-precision, time-critical ﬂight control.},
	language = {en},
	urldate = {2020-02-20},
	journal = {arXiv:1804.04154 [cs]},
	author = {Koch, William and Mancuso, Renato and West, Richard and Bestavros, Azer},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.04154},
	keywords = {Computer Science - Robotics},
	file = {Koch et al. - 2018 - Reinforcement Learning for UAV Attitude Control.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/B4KZMYZU/Koch et al. - 2018 - Reinforcement Learning for UAV Attitude Control.pdf:application/pdf}
}

@article{belousov_entropic_2019,
	title = {Entropic {Regularization} of {Markov} {Decision} {Processes}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/21/7/674},
	doi = {10.3390/e21070674},
	abstract = {An optimal feedback controller for a given Markov decision process (MDP) can in principle be synthesized by value or policy iteration. However, if the system dynamics and the reward function are unknown, a learning agent must discover an optimal controller via direct interaction with the environment. Such interactive data gathering commonly leads to divergence towards dangerous or uninformative regions of the state space unless additional regularization measures are taken. Prior works proposed bounding the information loss measured by the Kullback\&ndash;Leibler (KL) divergence at every policy improvement step to eliminate instability in the learning dynamics. In this paper, we consider a broader family of f-divergences, and more concretely    \&alpha;   -divergences, which inherit the beneficial property of providing the policy improvement step in closed form at the same time yielding a corresponding dual objective for policy evaluation. Such entropic proximal policy optimization view gives a unified perspective on compatible actor-critic architectures. In particular, common least-squares value function estimation coupled with advantage-weighted maximum likelihood policy improvement is shown to correspond to the Pearson     \&chi; 2    -divergence penalty. Other actor-critic pairs arise for various choices of the penalty-generating function f. On a concrete instantiation of our framework with the    \&alpha;   -divergence, we carry out asymptotic analysis of the solutions for different values of    \&alpha;    and demonstrate the effects of the divergence function choice on common standard reinforcement learning problems.},
	language = {en},
	number = {7},
	urldate = {2020-02-20},
	journal = {Entropy},
	author = {Belousov, Boris and Peters, Jan},
	month = jul,
	year = {2019},
	keywords = {\textit{f}-divergence, actor-critic methods, KL control, maximum entropy reinforcement learning},
	pages = {674},
	file = {Full Text PDF:/home/henry/snap/zotero-snap/10/Zotero/storage/HLT89SGC/Belousov und Peters - 2019 - Entropic Regularization of Markov Decision Process.pdf:application/pdf;Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/7NJ5LEPS/htm.html:text/html}
}

@article{lohner_towards_2019,
	title = {Towards overcoming the {LES} crisis},
	volume = {33},
	issn = {1061-8562, 1029-0257},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618562.2019.1612052},
	doi = {10.1080/10618562.2019.1612052},
	abstract = {Industrial large-eddy simulation (LES), i.e. overnight runs with O(109) degrees of freedom (DOFs) and O(107) timesteps, has been one of the top priorities of CFD research over the last two decades. Current network and solver timings indicate that the required target of 5 ms/timestep is within reach: some cumulant Lattice Boltzmann Method (LBM) codes are reporting speeds in this range using multi-GPU systems, while some Finite Difference Method (FDM) codes running on traditional CPUs are achieving speeds that are only a factor of 2–4 slower. This implies that the long wait may be coming to a close. During the last three decades, a number of promising approaches have been tried. Given that the majority of these were promoted as the ‘solution to LES’ or the ‘solution to turbulence’, the paper lists them under the label of ‘the false prophecies’. Furthermore, some of the assumptions that are always part of the scientific discovery process turned out to be incorrect. These are listed under the term ’the false assumptions’. From an informal survey conducted in January of 2018, it appears that simple Cartesian Finite Difference codes or, equivalently, Lattice Boltzmann codes may be the first to achieve industrial LES.},
	language = {en},
	number = {3},
	urldate = {2020-02-23},
	journal = {International Journal of Computational Fluid Dynamics},
	author = {Löhner, Rainald},
	month = mar,
	year = {2019},
	pages = {87--97},
	file = {Löhner - 2019 - Towards overcoming the LES crisis.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/UUT52PIK/Löhner - 2019 - Towards overcoming the LES crisis.pdf:application/pdf}
}

@article{verma_efficient_2018,
	title = {Efficient collective swimming by harnessing vortices through deep reinforcement learning},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/23/5849},
	doi = {10.1073/pnas.1800923115},
	abstract = {Fish in schooling formations navigate complex flow fields replete with mechanical energy in the vortex wakes of their companions. Their schooling behavior has been associated with evolutionary advantages including energy savings, yet the underlying physical mechanisms remain unknown. We show that fish can improve their sustained propulsive efficiency by placing themselves in appropriate locations in the wake of other swimmers and intercepting judiciously their shed vortices. This swimming strategy leads to collective energy savings and is revealed through a combination of high-fidelity flow simulations with a deep reinforcement learning (RL) algorithm. The RL algorithm relies on a policy defined by deep, recurrent neural nets, with long–short-term memory cells, that are essential for capturing the unsteadiness of the two-way interactions between the fish and the vortical flow field. Surprisingly, we find that swimming in-line with a leader is not associated with energetic benefits for the follower. Instead, “smart swimmer(s)” place themselves at off-center positions, with respect to the axis of the leader(s) and deform their body to synchronize with the momentum of the oncoming vortices, thus enhancing their swimming efficiency at no cost to the leader(s). The results confirm that fish may harvest energy deposited in vortices and support the conjecture that swimming in formation is energetically advantageous. Moreover, this study demonstrates that deep RL can produce navigation algorithms for complex unsteady and vortical flow fields, with promising implications for energy savings in autonomous robotic swarms.},
	language = {en},
	number = {23},
	urldate = {2020-02-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Verma, Siddhartha and Novati, Guido and Koumoutsakos, Petros},
	month = jun,
	year = {2018},
	pmid = {29784820},
	keywords = {autonomous navigation, deep reinforcement learning, energy harvesting, fish schooling, recurrent neural networks},
	pages = {5849--5854},
	file = {Full Text PDF:/home/henry/snap/zotero-snap/10/Zotero/storage/3R7M3HES/Verma et al. - 2018 - Efficient collective swimming by harnessing vortic.pdf:application/pdf;Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/CDL59TRS/5849.html:text/html}
}

@article{asmuth_actuator_2019,
	title = {Actuator {Line} {Simulations} of {Wind} {Turbine} {Wakes} {Using} the {Lattice} {Boltzmann} {Method}},
	issn = {2366-7443},
	url = {https://www.wind-energ-sci-discuss.net/wes-2019-94/},
	doi = {https://doi.org/10.5194/wes-2019-94},
	abstract = {{\textless}p{\textgreater}{\textless}strong{\textgreater}Abstract.{\textless}/strong{\textgreater} The presented work investigates the potential of large-eddy simulations (LES) of wind turbine wakes using the cumulant lattice Boltzmann method (CLBM). The wind turbine is represented by the actuator line model (ALM) that is implemented in a GPU-accelerated (Graphics Processing Unit) lattice Boltzmann framework. The implementation is validated and discussed by means of a code-to-code comparison to an established finite-volume Navier-Stokes solver. To this end, the ALM is subjected to a uniform laminar inflow while a standard Smagorinsky sub-grid scale model is employed in both numerical approaches. The comparison shows a good agreement in terms of the blade loads and near-wake characteristics. The main differences are found in the point of laminar-turbulent transition of the wake and the resulting far-wake. In line with other studies these differences can be attributed to the different orders of accuracy of the two methods. In a second part the possibilities of implicit LES with the CLBM are investigated using a limiter applied to the third-order cumulants in the scheme's collision operator. The study shows that the limiter generally ensures numerical stability. Nevertheless, a universal tuning approach for the limiter appears to be required, especially for perturbation-sensitive transition studies. In summary, the range of discussed cases outline the general feasibility of wind turbine simulations using the CLBM. In addition, it highlights the potential of GPU-accelerated LBM implementations to significantly speed up LES in the field of wind energy.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2020-02-23},
	journal = {Wind Energy Science Discussions},
	author = {Asmuth, Henrik and Olivares-Espinosa, Hugo and Ivanell, Stefan},
	month = dec,
	year = {2019},
	pages = {1--33},
	file = {Full Text PDF:/home/henry/snap/zotero-snap/10/Zotero/storage/HCDDVLKJ/Asmuth et al. - 2019 - Actuator Line Simulations of Wind Turbine Wakes Us.pdf:application/pdf;Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/9A3PZV9R/wes-2019-94.html:text/html}
}

@article{asmuth_actuator_2019-1,
	title = {The {Actuator} {Line} {Model} in {Lattice} {Boltzmann} {Frameworks}: {Numerical} {Sensitivity} and {Computational} {Performance}},
	volume = {1256},
	issn = {1742-6588, 1742-6596},
	shorttitle = {The {Actuator} {Line} {Model} in {Lattice} {Boltzmann} {Frameworks}},
	url = {https://iopscience.iop.org/article/10.1088/1742-6596/1256/1/012022},
	doi = {10.1088/1742-6596/1256/1/012022},
	abstract = {The growing use of large-eddy simulations for the modelling of wind farms makes the need for eﬃcient numerical frameworks more essential than ever. GPU-accelerated implementations of the Lattice Boltzmann Method (LBM) have shown to provide signiﬁcant performance gains over classical Navier-Stokes-based computational ﬂuid dynamics. Yet, their use in the ﬁeld of wind energy remains limited to date. In this fundamental study the cumulant LBM is scrutinised for actuator line simulations of wind turbines. The numerical sensitivity of the method in a simple uniform inﬂow is investigated with respect to spatial and temporal resolution as well as the width of the actuator line’s regularisation kernel. Comparable accuracy and slightly better stability properties are shown in relation to a standard Navier-Stokes implementation. The results indicate the overall suitability of the cumulant LBM for wind turbine wake simulations. The potential of the LBM for future wind energy applications is clariﬁed by means of a brief comparison of computational performance.},
	language = {en},
	urldate = {2020-02-23},
	journal = {Journal of Physics: Conference Series},
	author = {Asmuth, Henrik and Olivares-Espinosa, Hugo and Nilsson, Karl and Ivanell, Stefan},
	month = jul,
	year = {2019},
	pages = {012022},
	file = {Asmuth et al. - 2019 - The Actuator Line Model in Lattice Boltzmann Frame.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/MZ26LY4W/Asmuth et al. - 2019 - The Actuator Line Model in Lattice Boltzmann Frame.pdf:application/pdf}
}

@misc{noauthor_impact_nodate,
	title = {Impact of collision models on the physical properties and the stability of lattice {Boltzmann} methods},
	url = {https://www.researchgate.net/publication/339136823_Impact_of_collision_models_on_the_physical_properties_and_the_stability_of_lattice_Boltzmann_methods},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2020-02-23},
	journal = {ResearchGate},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/MXFJNTTH/339136823_Impact_of_collision_models_on_the_physical_properties_and_the_stability_of_lattice_Bo.html:text/html}
}

@misc{noauthor_cumulant_nodate,
	title = {The cumulant lattice {Boltzmann} equation in three dimensions: {Theory} and validation {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {The cumulant lattice {Boltzmann} equation in three dimensions},
	url = {https://reader.elsevier.com/reader/sd/pii/S0898122115002126?token=C13686A8F2F4B133E6C9C5502765A7572500249D4C8ECFA02B34A3A2EF9532BBF97157C7DE8A908F58166F1A7CD2A4CD},
	language = {en},
	urldate = {2020-02-23},
	doi = {10.1016/j.camwa.2015.05.001},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/H3IRGCZ5/S0898122115002126.html:text/html;Volltext:/home/henry/snap/zotero-snap/10/Zotero/storage/WFIFY8WM/The cumulant lattice Boltzmann equation in three d.pdf:application/pdf}
}

@article{geier_fourth_2018,
	title = {Fourth order {Galilean} invariance for the lattice {Boltzmann} method},
	volume = {166},
	issn = {0045-7930},
	url = {http://www.sciencedirect.com/science/article/pii/S0045793018300239},
	doi = {10.1016/j.compfluid.2018.01.015},
	abstract = {Using the structure of a recursive asymptotic analysis we derive conditions on cumulants that guarantee a prescribed order of Galilean invariance for lattice Boltzmann models. We then apply these conditions to three different lattice Boltzmann models and obtain three models with fourth order accurate advection. One of the models uses 27 speeds on a body centered cubic lattice, one uses 33 speeds on an extended Cartesian lattice and one uses 27 speeds plus three finite differences on a Cartesian lattice. All models offer too few degrees of freedom to impose the conditions on the cumulants directly. However, the specific aliasing structure of these lattices permit fourth order accuracy for a model specific optimal reference temperature. Our theoretical derivations are confirmed by measuring the phase lag of traveling vortexes and shear waves.},
	language = {en},
	urldate = {2020-02-24},
	journal = {Computers \& Fluids},
	author = {Geier, Martin and Pasquali, Andrea},
	month = apr,
	year = {2018},
	keywords = {Crystallographic lattice Boltzmann, Cumulants, Fourth order, Galilean invariance, Lattice Boltzmann, Recursive asymptotic analysis},
	pages = {139--151},
	file = {ScienceDirect Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/HSEWF89I/S0045793018300239.html:text/html;ScienceDirect Full Text PDF:/home/henry/snap/zotero-snap/10/Zotero/storage/KPDNAPR6/Geier und Pasquali - 2018 - Fourth order Galilean invariance for the lattice B.pdf:application/pdf}
}

@article{ciri_large-eddy_2017,
	title = {Large-eddy simulations with extremum-seeking control for individual wind turbine power optimization},
	volume = {20},
	copyright = {Copyright © 2017 John Wiley \& Sons, Ltd.},
	issn = {1099-1824},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/we.2112},
	doi = {10.1002/we.2112},
	abstract = {Large-eddy simulations of the flow past an array of three aligned turbines have been performed. The study is focused on below rated (Region 2) wind speeds. The turbines are controlled through the generator torque gain, as usually done in Region 2. Two operating strategies are considered: (i) preset individual optimum torque gain based on a model for the power coefficient (baseline case) and (ii) real-time optimization of torque gain for maximizing each individual turbine power capture during operation. The real-time optimization is carried out through a model-free approach, namely, extremum-seeking control. It is shown that ESC is capable of increasing the power production of the array by 6.5\% relative to the baseline case. The extremum-seeking control reduces the torque gain of the downstream turbines, thus increasing the angular speed of the blades. This results in improved aerodynamics near the tip of the blade that is the portion contributing mostly to the torque and power. In addition, an increase in angular speed leads to a larger entrainment in the wake, which also contributes to provide additional available power downstream. It is also shown that the tip speed ratio may not be a reliable performance indicator when the turbines are in waked conditions. This may be a concern when using optimal parameter settings, determined from isolated turbine models, in applications with waked turbines. Copyright © 2017 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {9},
	urldate = {2020-02-24},
	journal = {Wind Energy},
	author = {Ciri, Umberto and Rotea, Mario and Santoni, Christian and Leonardi, Stefano},
	year = {2017},
	keywords = {extremum-seeking control, large-eddy simulations, model-free optimization, wind energy, wind farm control},
	pages = {1617--1634},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/HJTR8AFF/we.html:text/html;Full Text PDF:/home/henry/snap/zotero-snap/10/Zotero/storage/I9P4UU8P/Ciri et al. - 2017 - Large-eddy simulations with extremum-seeking contr.pdf:application/pdf}
}

@article{belus_exploiting_2019,
	title = {Exploiting locality and translational invariance to design effective deep reinforcement learning control of the 1-dimensional unstable falling liquid film},
	volume = {9},
	issn = {2158-3226},
	url = {http://aip.scitation.org/doi/10.1063/1.5132378},
	doi = {10.1063/1.5132378},
	abstract = {Instabilities arise in a number of flow configurations. One such manifestation is the development of interfacial waves in multiphase flows, such as those observed in the falling liquid film problem. Controlling the development of such instabilities is a problem of both academic interest and industrial interest. However, this has proven challenging in most cases due to the strong nonlinearity and high dimensionality of the underlying equations. In the present work, we successfully apply Deep Reinforcement Learning (DRL) for the control of the one-dimensional depth-integrated falling liquid film. In addition, we introduce for the first time translational invariance in the architecture of the DRL agent, and we exploit locality of the control problem to define a dense reward function. This allows us to both speed up learning considerably and easily control an arbitrary large number of jets and overcome the curse of dimensionality on the control output size that would take place using a naïve approach. This illustrates the importance of the architecture of the agent for successful DRL control, and we believe this will be an important element in the effective application of DRL to large two-dimensional or three-dimensional systems featuring translational, axisymmetric, or other invariance.},
	language = {en},
	number = {12},
	urldate = {2020-02-25},
	journal = {AIP Advances},
	author = {Belus, Vincent and Rabault, Jean and Viquerat, Jonathan and Che, Zhizhao and Hachem, Elie and Reglade, Ulysse},
	month = dec,
	year = {2019},
	pages = {125014},
	file = {Belus et al. - 2019 - Exploiting locality and translational invariance t.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/55PHPYF6/Belus et al. - 2019 - Exploiting locality and translational invariance t.pdf:application/pdf}
}

@article{garnier_review_2019,
	title = {A review on {Deep} {Reinforcement} {Learning} for {Fluid} {Mechanics}},
	url = {http://arxiv.org/abs/1908.04127},
	abstract = {Deep reinforcement learning (DRL) has recently been adopted in a wide range of physics and engineering domains for its ability to solve decision-making problems that were previously out of reach due to a combination of non-linearity and high dimensionality. In the last few years, it has spread in the field of computational mechanics, and particularly in fluid dynamics, with recent applications in flow control and shape optimization. In this work, we conduct a detailed review of existing DRL applications to fluid mechanics problems. In addition, we present recent results that further illustrate the potential of DRL in Fluid Mechanics. The coupling methods used in each case are covered, detailing their advantages and limitations. Our review also focuses on the comparison with classical methods for optimal control and optimization. Finally, several test cases are described that illustrate recent progress made in this field. The goal of this publication is to provide an understanding of DRL capabilities along with state-of-the-art applications in fluid dynamics to researchers wishing to address new problems with these methods.},
	language = {en},
	urldate = {2020-02-25},
	journal = {arXiv:1908.04127 [physics]},
	author = {Garnier, Paul and Viquerat, Jonathan and Rabault, Jean and Larcher, Aurélien and Kuhnle, Alexander and Hachem, Elie},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.04127},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Fluid Dynamics},
	file = {Garnier et al. - 2019 - A review on Deep Reinforcement Learning for Fluid .pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/XEJHAAFJ/Garnier et al. - 2019 - A review on Deep Reinforcement Learning for Fluid .pdf:application/pdf}
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f\&v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
	language = {en},
	number = {5},
	urldate = {2020-02-25},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366},
	file = {Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/AXUPG4QQ/Hornik et al. - 1989 - Multilayer feedforward networks are universal appr.pdf:application/pdf}
}

@article{duraisamy_turbulence_2019,
	title = {Turbulence {Modeling} in the {Age} of {Data}},
	volume = {51},
	issn = {0066-4189},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-fluid-010518-040547},
	doi = {10.1146/annurev-fluid-010518-040547},
	abstract = {Data from experiments and direct simulations of turbulence have historically been used to calibrate simple engineering models such as those based on the Reynolds-averaged Navier–Stokes (RANS) equations. In the past few years, with the availability of large and diverse data sets, researchers have begun to explore methods to systematically inform turbulence models with data, with the goal of quantifying and reducing model uncertainties. This review surveys recent developments in bounding uncertainties in RANS models via physical constraints, in adopting statistical inference to characterize model coefficients and estimate discrepancy, and in using machine learning to improve turbulence models. Key principles, achievements, and challenges are discussed. A central perspective advocated in this review is that by exploiting foundational knowledge in turbulence modeling and physical constraints, researchers can use data-driven approaches to yield useful predictive models.},
	number = {1},
	urldate = {2020-02-25},
	journal = {Annual Review of Fluid Mechanics},
	author = {Duraisamy, Karthik and Iaccarino, Gianluca and Xiao, Heng},
	month = jan,
	year = {2019},
	pages = {357--377},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/RAWSGIDV/annurev-fluid-010518-040547.html:text/html;Eingereichte Version:/home/henry/snap/zotero-snap/10/Zotero/storage/F4Q2W6PK/Duraisamy et al. - 2019 - Turbulence Modeling in the Age of Data.pdf:application/pdf}
}

@misc{noauthor_windrad_nodate,
	title = {Windrad sucht {Standort}},
	url = {https://katapult-magazin.de/de/artikel/artikel/fulltext/windrad-sucht-standort/},
	language = {en},
	urldate = {2020-02-26},
	file = {Snapshot:/home/henry/snap/zotero-snap/10/Zotero/storage/APRP4LSM/windrad-sucht-standort.html:text/html}
}

@article{cheung_simple_2016,
	title = {A {Simple} {Model} for the {Turbulence} {Intensity} {Distribution} in {Atmospheric} {Boundary} {Layers}},
	volume = {753},
	issn = {1742-6588, 1742-6596},
	url = {http://stacks.iop.org/1742-6596/753/i=3/a=032008?key=crossref.08833531de88c74ec85d8b2a20a8d0bb},
	doi = {10.1088/1742-6596/753/3/032008},
	abstract = {In the current work, we examine the distribution of turbulence intensity (TI) in simulations of atmospheric boundary layers (ABL) for wind turbine applications. We relate the turbulent fluctuations to the mean wind shear profile for neutrally stable ABL based on turbulent boundary layer theory. The results are then compared to corresponding LES data for various shear conditions. From this, a model for the TI distribution across a rotor disc is devised and calibrated. In addition, a second simpler version of the model is derived by assuming a power law for the velocity profile. The models are related to the Mann model and shown to have a similar scaling, however they are simpler and based on assumptions for turbulent boundary layers rather than homogeneous turbulence. The models are then tested for a range of stability conditions and shear values of the ABL including waked conditions from upstream wind turbines that stretch the assumptions made.},
	language = {en},
	urldate = {2020-02-26},
	journal = {Journal of Physics: Conference Series},
	author = {Cheung, L C and Premasuthan, S and Davoust, S and von Terzi, D},
	month = sep,
	year = {2016},
	pages = {032008},
	file = {Cheung et al. - 2016 - A Simple Model for the Turbulence Intensity Distri.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/WEHX7MKB/Cheung et al. - 2016 - A Simple Model for the Turbulence Intensity Distri.pdf:application/pdf}
}

@article{schulman_trust_2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justiﬁed procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	language = {en},
	urldate = {2020-02-27},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	file = {Schulman et al. - 2017 - Trust Region Policy Optimization.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/53JD2RTH/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf:application/pdf}
}

@article{williams_simple_nodate,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associativereinforcementlearning algorithmsfor connectionist networkscontainingstochasticunits. These algorithms,calledREINFORCEalgorithms,are shownto makeweight adjustmentsin a direction that lies alongthe gradient of expectedreinforcementin both immediate-reinforcement tasks and certain limited forms of delayed-reinforcementtasks, and they do this without explicitly computing gradient estimatesor even storing informationfrom which such estimates couldbe computed. Specificexamples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novelbut potentiallyinterestingin their own right. Also givenare resultsthat showhow such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerationsthat might be used to help developsimilar but potentially more powerfulreinforcement learning algorithms.},
	language = {en},
	author = {Williams, Ronald J},
	pages = {28},
	file = {Williams - Simple statistical gradient-following algorithms f.pdf:/home/henry/snap/zotero-snap/10/Zotero/storage/E6M78RML/Williams - Simple statistical gradient-following algorithms f.pdf:application/pdf}
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	language = {en},
	number = {3-4},
	urldate = {2020-02-27},
	journal = {Machine Learning},
	author = {Williams, Ronald J.},
	month = may,
	year = {1992},
	pages = {229--256},
	file = {Volltext:/home/henry/snap/zotero-snap/10/Zotero/storage/XK59CQFW/Williams - 1992 - Simple statistical gradient-following algorithms f.pdf:application/pdf}
}