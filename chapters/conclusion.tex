% !TEX root = ../Diploma.tex
In this work reinforcement learning was applied to control of a wind farm simulation utilizing the actuator line method and LES-LBM in order to increase the total generated power of the farm. Two approaches were tested. First, parameters of an already existing dynamic control strategy were improved. This was very successful, with an increase in total generated power by up to $18.2$ percent. Additionally, further insight into the physical phenomena of the dynamic control was gained. Second, the agent controlled the turbine directly. This approach was not able to improve total generated power and possible reasons for this were identified, among them the separation in time and space of the turbines. A possible solution of this problem by using multiple agents and environments was proposed. \\
In a first step, the theoretical basis for RL, ANNs, turbine modelling and control as well as LBM was laid. Also a more complete deduction for a second order refinement scheme for the cumulant LBM was given, which was not available in the literature. Furthermore a literature review of advanced dynamic induction control for wind farms and of active flow control via reinforcement learning was conducted. It was found, that the coupling of AFC and RL have only been conducted for laminar, two-dimensional problems. \\
In the second part, the framework developed for this work as well as details of the implementation and design choices were explained. Furthermore, the setup of the simulations was explained. The wind park features three turbines with a distance of five rotor diameters and turbulent inflow with a turbulence intensity of five percent. The different agents that were trained were also explained. Two agents were trained to improve the helix control strategy, which was taken from literature. One of the agents had a network of three layers with a width of 400 nodes, the other agents network consists of three layers with a width of ten nodes. Three agents were trained to learn a new control strategy by directly controlling the generator torque of the turbines. The training different in the length of the episodes, the first agent was trained with episodes of 1500 actions, the second and the third agent with episodes containing 500 actions. The return for the first and the second agent was discounted with a discount rate of $\gamma=0.99$, while the return for the third agent was discounted with a discount rate of $\gamma=0.95$. \\
Finally the results were discussed. First the implementation of the different parts of the simulations were validated. Then preliminary studies with a simple aerodynamic model of a single turbine were conducted. They showed that the training of an agent requires $\mathcal{O}(10^6\si{s})$ of simulated time. The results from the optimization of the parameters of the helix control strategy were discussed. The two agents resulted in similar behaviour with one difference. The agent with the smaller network set a negative amplitude, resulting in a phase shift of $\SI{180}{\degree}$ of the wake helix of the first turbine. With this shift an increase of $\SI{18.2}{}$ percent in total generated power was found, while the other agent, which did not feature the phase shift, resulted in an increase of $\SI{6.75}{}$ percent. It was found that this difference in total generated power was caused by a phase shift of the helix caused by the first turbine and the helix caused by the second turbine. In the first case, the helices were aligned, resulting in an increase of the radius of the helix. In the second case, the helices are shifted by $\SI{180}{\degree}$, which results in a destruction of the helix after the second turbine, decreasing power production at the second and the third turbine. Based on this finding it was proposed to shift the helix of the second turbine to align with the first turbine, which found an increase to that of the park controlled by agent with the small network. This provided further insight into the physical phenomena governing the helix approach, which had only been tested for a two-turbine park, of which only the first turbine was controlled with the helix approach. The findings in this work show how an application to a larger park can be offer significant increases in generated power.  \\
Lastly, the results of the training of the three agents directly controlling the generator torque of the turbines were evaluated. None of the agents were able to develop a strategy that performed better than a greedy-controlled park. Still, the analysis of the flow in a park controlled by one of the agents showed, that a slower reacting greedy controller might be able to reduce fluctuations in generated power. Three possible reasons were found why the agents could not find a good strategy in the current setup. All three agents developed a static strategy and did not react to the state of the environment any more. This was attributed to an inadequate transformation of the actions of the agent to the control quantities of the turbine. Three solutions were proposed. First, the a-priori values used for the transformation could be improved by using estimates based on greedy control. Second, an additional layer of linear nodes could be used in the network. Third, the agent could be trained to behave like a greedy controller by supervised learning. The second and third problem identified are related to the reward. On the one hand, the reward, which was defined to be the sum of the generated power of the three turbines, favours improving the first turbine, since this turbine produces the most power. Second, the control of the first turbine influences the second and third turbine but only after a long period of time. During this period of time, the control has no influence on the current state of the environment. If the influence on the downstream turbines is captured by the reward, it also includes many other time steps. If it is not captured, the optimization goal reduces to a greedy strategy. To tackle both of these problems it was proposed to change the setup to the use of a separate environment and agent for each turbine as well as modifying the the reward to include only time steps which can be relevant. \\
Three main conclusions regarding the application of RL to AFC can be drawn from this work. First, reinforcement learning coupled to active flow control requires a lot of simulated time, to apply it to fully turbulent flows therefore is computationally very expensive. The cumulant LBM with the extension of second order refinement is a method well-suited for this task. Additionally, the hardware requirements of RL as well as LBM are similar, since both can be greatly accelerated by the use of GPUs. Second, applying RL to realistic problems requires many design choices from the great number of parameters in the learning algorithm to details in the implementation regarding non-dimensionalization. Since this was the first application of RL to such a realistic problem, no literature was available to rely on. Third, this methodology has the potential of discovering new control strategies or improving already existing strategies as well as hinting researchers at physical phenomena, which where not previously observed. \\
The results regarding the helix approach showed, that it has great potential to increase the power production of small wind parks but the newly gained understanding of the physics of this approach suggest that it can easily be extended to larger parks, if the phase shift of the helices is accounted for. To further prove the feasibility of this approach for control of real-world wind parks, simulations including a sheared boundary layer will have to be conducted, as the shear greatly influences the movement of the wake. Additionally a thorough study of the loads will be necessary, as they have only been assessed through thrust and aerodynamic moment until this point. \\
Future works on applying RL to wind farm control will have to solve the problems discussed above. Especially the problem of time shifted actions will have to be addressed to turn RL into a more useful tool for a broader range of problems. A possible solution to this was already suggested in this chapter. It might also be necessary to gain further understanding of RL in turbulent flows by studying simpler problems, such as a single turbine. The analysis of the control strategy developed by the RL-agent is further hindered by the black-box character of the neural network. To this end it might be feasible to use different approximation functions for the policy.