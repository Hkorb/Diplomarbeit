% !TEX root = ../../Diploma.tex
\section{Implementation}
\subsection{Implementation Strategy}
In this work the performance of two controllers in two simulation environments is studied. As a baseline case a greedy controller is implemented. This is compared to a controller that is governed by an RL-agent. They are compared in a one-dimensional BEM model of a turbine, which is used for a more rapid development and an initial parameter study, and an LBM-ALM environment with multiple turbines. Everything is implemented in Python with the exception of the LBM-ALM, which is implemented in C++.\\
In contrast to most other applications of machine learning, in this case the main program is not the one governing the machine learning, but rather the simulation environment. This is due to the fact that the LBM-ALM simulation environment already existed prior to this work and it was expected to be simpler to design the RL-controller so that it could be called by the simulation environment, rather than breaking up the already existing code. Furthermore the LBM-ALM consumes the vast amount of computational time, so any potential for optimization in this part should be used. However, this approach also created some difficulties, especially regarding the concurrency of events and scope of objects, many of which were solved by using techniques of object oriented programming.
\subsection{RL-Controller}
The RL-Controller relies on the library TF-Agents \cite{guadarrama_tf-agents_2018}, which implements many RL algorithms and is based on the Python API of TensorFlow \cite{abadi_tensorflow_2015}. The controller implements an MDP, with an environment and an agent and also governs the interaction between them. The frequency of interactions is not related to the size of the timestep of the simulation environment, since the relevant time scales are not necessarily connected \cite{rabault_deep_2018}. It is governed by a parameter, the number of timesteps per action, $N_{t,A}$. \\
An instance of the class \texttt{PPOAgent} represents the agent and contains the methods to calculate actions based on the observations and to train the agent. It is based on the PPO-algorithm explained in \ref{sec:pgm}. The actor network consists of three layers, of which the first layer is either a fully connected LSTM cell or a regular feed forward layers The activation function of these layers is $\tanh$. They have the same width, which is adjustable. The last layer is a feed forward layer of $\mathrm{softplus}$ nodes or $\tanh$, depending on the controlled quantity. The value network is a three layered feed forward network with a $\mathrm{softplus}$ activation. \\
The environment is implemented in the class \texttt{TurbineEnvironment}. The action $\mathsf{a}$ provided by the agent is smoothed with an exponential filter
\begin{equation}
\hat{\mathsf{\vec{a}}}_t = \hat{\mathsf{\vec{a}}}_{t-1} + \alpha_f(\mathsf{\vec{a}}_t - \hat{\mathsf{\vec{a}}}_{t-1}) \label{eq:exp_filter}
\end{equation}
to make it continuous as done for example in \cite{rabault_deep_2018}. The decay rate $\alpha_f$ is $\alpha_f = r_f^{1/N_{t,A}}$ with a forget ratio $r_f$ of \SI{0.99}{}. $\hat{\mathsf{\vec{a}}}$ is applied as control of the turbine. To allow for more refined action space, the value of the control variables is calculated by
\begin{equation}
\varphi(t) = \varphi_{base} + \sum^{N_a}_{i=0} A_{\varphi, i} \cos(2 \pi f_{\phi,i}t), \label{control_amplitude}
\end{equation}
with $\varphi$ being the controllable quantity, $\varphi_{base}$ is a base value that is modulated by $N_a$ cosine waves with frequencies $f_{\varphi,i}$ and amplitudes $A_{\phi,i}$. Unless controlled by the agent, these quantities are set according to the greedy control strategy, that is all amplitudes and base values are zero except the base-value of $M_{gen}$. \\
The observations and reward are also filtered exponentially, with the same rate as the action. Observations can include quantities from turbines, such as $\omega$, but also velocities sampled from the flow-field. They can also be from multiple past timesteps in addition to the current timestep. If the environment reaches a terminal state, because a rotor turns too slow or too fast, the environment is controlled by a greedy controller, until it is in a stable state again. This was implemented since a reinitialization of the LBM-ALM simulation environment would be very costly. To reduce wall time, multiple environments can be run at the same time, each represented by a single instance of \texttt{TurbineEnvironment}. Therefore, all the TurbineEnvironments are wrapped in a single instance of a \texttt{ParallelTurbineEnvironment}. \\
\subsection{Greedy Controller}
The greedy controller is an implementation of the baseline generator-torque controller given in \cite{jonkman_definition_2009}, which is based on the greedy control strategy explained in \ref{sec:WTC}. To simplify the calculations the controllable torque is the torque at the rotor, losses due to the transmission are ignored. The proportionally constant $\kappa$ was optimized using the BEM environment and found to be $\SI{1.268e6}{kgm^2}$.
\subsection{LBM-ALM Environment}
The LBM-ALM environment is based on the work of Asmuth et al. \cite{asmuth_actuator_2020, asmuth_actuator_2019}. The actuator line model is implemented in elbe, a cumulant LBM code  utilizing graphical processing units (GPU) \cite{jansen_validation_2015, gehrke_scrutinizing_2017}. For this work the code was extended to include a second order accurate refinement scheme according to Sch√∂nherr et al. \cite{schonherr_towards_2015}, for further information the reader is referred to \autoref{app:refinement}. Multiple independent domains can be instantiated in one call to the main function, with creating only one instance of the controller, allowing for a significant reduction in wall time \cite{rabault_accelerating_2019}. The inlet is a uniform inflow, that can be superimposed with fluctuations of a Mann box \cite{mann_wind_1998}, created by a synthetic turbulence generator. The outlet uses a sponge layer, as described in \autoref{sec:LBM}.
\subsection{Fast Implimentation of a BEM Environment}
To provide the physical inputs of a one-dimensional model of the flow field and the turbine a steady state BEM-code for the computation of $C_t$ and $C_n$ was implemented. To minimize computational time, a table of 400 values of $M_{aero}$ is precomputed and then linearly interpolated. The fluid field is composed of a base velocity and an optionally superimposed turbulent fluctuation of a Mann box, which is computed by a synthetic turbulence generator. The base velocity can also be varying in time, either by defining a sine wave or by regularly sampling a random velocity within a given interval.
\subsection{Interaction of Controller and Simulation Environment}
To provide a layer of abstraction between simulation environments and controllers, exchange of information takes place via the so-called visitors. Each turbine and probe is represented by an instance of a visitor. It is provided with input by the simulation environments such as $M_{aero}$ or the velocity, but can also set control variables such as $M_{gen}$. This allows for an easy recombination of simulation environments and controllers. \\
\begin{figure}[h]
	\centering
	\def\svgwidth{1 \textwidth}
	\input{pics/diagrams/rundiagram.pdf_tex}
	\caption{Schematic of a timestep sequence. To trace order of execution, follow solid arrows and go around ellipsis clockwise, dashed arrows represent exchange of information.}
	\label{fig:timestep}
\end{figure}
As an example for the interaction between the controller and the simulation environment, a timestep of a simulation with the ALM-LBM and a RL-Controller is given in \ref{fig:timestep}. It shows that the probes and turbines hold information but not execute methods themselves. It also shows the implementation languages. To simplify communication across languages, objects that exchange information across languages are mirrored in both languages. This includes the visitors as well as the controller.
