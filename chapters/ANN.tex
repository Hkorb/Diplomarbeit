\chapter{Machine Learning for a Continuous Control Problem}
\section{Reinforcement Learning}
\subsection{Markov Decision Process}
The mathematical formulation on which RL is based is the Markov Decision Process (MDP). Its two main components are the agent that given a state $S_t$ takes an action $A_t$, and the environment, that responds to the action $A_t$ with changing its state to $S_{t+1}$ and giving feedback to the agent in form of a reward $R_t$. The interaction takes place at discrete time steps $t$ and the sequence of state, action and reward is referred to as the trajectory. The dynamics of the MDP are described by the function $p$ that is defined in \eqref{eq:prob_func}. It defines the probability of the state $s'$ with reward $r$ occuring, given the state $s$ and action $a$. Note that the following derivations will be constricted to finite MDPs, meaning that state and action space are discrete, however the concepts all are transferable to continuous action and state space.
\begin{align}
	p(s',r \vert s,a) \doteq Pr(S_t=s', R_t=r \vert S_{t-1} = s, A_{t-1} = a) \label{eq:prob_func}
\end{align}
For a process to be a Markov Decision Process, the $p$ must only depent on $s$ and $a$. Therefore $s$ must include all information necessary to determine the future behaviour of the environment. This is not limited to information currently present in the environment, when thinking of this in terms of the wind farm problem at hand, the state could include data about wind speeds at the current time but also from time steps in the past. This approach allows to model virtually any interaction as a MDP, simply by including every bit of information from the beginning of time into the state. Obviously this is not feasible and therefore a careful choice of the information in the state is necessary. \\
The goal of the learning process is to maximize the sum of the rewards in the long run. Therefore a new quantity is defined, the return $G_t$ that includes not only $R_t$ but also the rewards received in future time steps. While in many applications of RL, the process naturally comes to an end, referred to as the terminal state $S_T$, in problems of continuous control this is not the case. Therefore the timeline is broken up into episodes. This allows for a finite computation of $G_t$. A typical formulation of $G_t$, referred to as a discounted return is given in \eqref{eq:return}. It includes a discount rate $\gamma$, that emphasizes rewards in the near future. If $\gamma = 0$, $G_t = R_t$, if $\gamma = 1$, the return is the sum of all future rewards. \cite[p. 47- 57]{sutton_reinforcement_2018}
\begin{align}
	G_t \doteq R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3}... = \sum_{t'=t}^{T}\gamma^{t'-t} R_{t'},\quad \gamma \in [0, 1] \label{eq:return}
\end{align}
Now the goal of the learning process is defined, but not what to learn. There exist three possible answers to this question: a model of the environment, a value function or a policy. Also combinations of these components is possible. In the case of continuous control, most common approaches are model-free, meaning either learning a value function, a policy or both. Therefore model-based methods will not be discussed further and the reader is referred to the book by Sutton and Barto \cite{sutton_reinforcement_2018}. \\
The policy $\pi$ is the mapping from states to actions with a set of adjustable parameters. Under the policy $\pi$ with its vector of parameters set to $\vec{\theta}$, the probability of Action $A_t=a$ given a state $S_t=s$ is denoted as $\pi_{\vec{\theta}}(a|s)$. The value function of a state $s$ under a policy $\pi$ is denoted as $v_{\pi}(s)$ and is the expected return if the agent acts according to $\pi$, starting from state $s$. Note that for convenience the parameters of $\pi$ were be dropped. For MDPs it can be defined as \eqref{eq:value_func}. 
\begin{align}
	v_{\pi} &\doteq \mathrm{E}_\pi \left[ G_t \vert S_t=s \right] =
	\mathrm{E}_\pi \left[\sum_{t'=t}^{T}\gamma^{t'-t} R_{t'} \vert S_t=s\right] \label{eq:value_func} \\
	&= \sum_{a} \pi(a \vert s) 
	\sum_{s'} \sum_{r} p(s',r \vert s,a) \left( r + \gamma v_\pi(s') \right) \label{eq:Bellmann}
\end{align}
In the form of \eqref{eq:Bellmann}, the equation is referred to as the Bellmann equation and its unique solution is the value function $v_\pi$. Analogously, the action-value function is the expected reward of taking action $a$ at state $s$ under the policy $\pi$, denoted by $q_\pi(s,a)$. It is defined by \eqref{eq:action_value_func}. \cite[p. 58-59]{sutton_reinforcement_2018}
\begin{align}
	q_\pi(s,a) \doteq \mathrm{E}_\pi \left[ G_t \vert S_t=s, A_t=a \right] =
	\mathrm{E}_\pi \left[\sum_{t'=t}^{T}\gamma^{t'-t} R_{t'} \vert S_t=s, A_t=a\right] \label{eq:action_value_func} 
\end{align}
\subsection{Policy Gradient Methods}
With a known value function, it is possible to construct a policy and by improving the value function, the policy can be improved. However, there exist advantages to directly improve the policy, especially for continuous state and action space. Policy gradient methods use the gradient of a perfomance measure $J(\vec{\theta})$ with respect to the parameters $\vec{\theta}$ of a policy. This gradient can be used in optimization algorithms, such as stochastic gradient descent \cite[p. 201]{sutton_reinforcement_2018} or its extensions such as Adam \cite{kingma_adam_2017}. In its basic form, SGD performs an update according to \ref{eq:sgd}. The parameter $\alpha$ is called the learning rate.
\begin{align}
	\vec{\theta}_{t+1} = \vec{\theta}_t + \alpha \nabla J(\vec{\theta}) \label{eq:sgd}
\end{align}
The policy gradient methods differ now only in the performance measure. The first such algorithm proposed is the REINFORCE algorithm \cite{williams_simple_1992}. Its definition of $\nabla J(\vec{\theta})$ is presented in \eqref{eq:reinforce}. It follows from the policy gradient theorem and substituting all values of $A$ and $S$ with the actions and states from one trajectory. Thus all the values necessary for the computation of the gradient are known. \cite[p.324-328]{sutton_reinforcement_2018}
\begin{align}
	\nabla J(\vec{\theta}) 
	&= \mathrm{E}_\pi \left[ \sum_{a} \pi_{\vec{\theta}} (a\vert S_t) q_\pi(S_t, a)
	\frac{\nabla \pi_{\vec{\theta}}(a\vert S_t)}{\pi_{\vec{\theta}}(a\vert S_t)} \right]\label{eq:reinforce} \\
	&= \mathrm{E}_\pi \left[G_t \frac{\nabla \pi_{\vec{\theta}}(A_t\vert S_t)}{\pi_{\vec{\theta}}(A_t\vert S_t)} \right] \label{eq:reinforce2}
\end{align}
While this algorithm makes a computation possible, it is inefficient. Therefore newer methods have been devised such as the Trust Region Policy Optimization (TRPO) \cite{schulman_trust_2017} and the Proximal Policy Optimization (ppo) \cite{schulman_proximal_2017}. They are closely related and both propose a surrogate performance measure that limits the size of the gradient to ensure monotonic improvement in the case of TRPO and a close approximate in the case of ppo. However, the computation of the surrogate in ppo is simpler and more efficient, which helped policy gradient methods to become one of the most used algorithms in continuous control problems. One version of the surrogate performance measure, which is also referred to as objective, is given in \eqref{eq:object_ppo}. The probability ratio $r_t$ compares the policy after the update to the policy before the update. Therefore $\pi_{\vec{\theta}}$ has to be approximated. $a_\pi(A_t \vert S_t)$ is an estimator of the advantage function, which is defined as $a_\pi(A_t \vert S_t) = q_\pi(A_t \vert S_t) - v_\pi(S_t)$. This estimator also has to be found, however, this is a regular optimization problem, which can be solved via SGD or Adam.
\begin{align}
	r_t(\vec{\theta}) &\doteq \frac{\pi_{\vec{\theta}}(A_t \vert S_t)}{\pi_{\vec{\theta}_{old}} (A_t \vert S_t)} \\
	J &\doteq \mathrm{E}_\pi \left[\min \left(r_t(\vec{\theta}) a_\pi(A_t \vert S_t), \mathrm{clip}(r_t(\vec{\theta}), 1- \epsilon, 1+\epsilon) a_\pi(A_t \vert S_t) \right) \right] \label{eq:object_ppo} \\
			\boldsymbol{\mathrm{a}}
\end{align}
In addition to the ratio probability clipping, other regularizations can be added to the objective function, for example an $l_2$ regularization, that adds a penalty proportional to the size of $\vec{\theta}$.



\section{Artificial Neural Networks}
\subsection{Feed-forward networks}
In the section above, a way to update parameters of the policy or value function were described, however, no description of the function itself was given. In principal any function with a set of parameters can be used, but usually, an artificial neural network (ANN) is used. ANNs are comprised of layers of neurons. More precisely a layer $k$ of $S$ neurons takes as input a vector $\vec{p}$ of length $R$. The output of the layer is a new vector $a$, wich then is the input for the next layer of the network. In a simple feed-forward layer output $a_j^k$ of the j-th entry in $a^k$, the output of the k-th layer of the network, is computed according to \eqref{eq:neuron}. The entries $w_{i,j}^k$ of the matrix $\vec{W^k}$ are called the weights of the layer and the entries $b_j^k$ of the vector $\vec{b}^k$ are called its bias. $f$ is called the activation function, which can be chosen freely, but typical choices include the hyperbolic tangent (tanh), the sigmoid-function ($\sigma$) or the rectified linear unit ($\mathrm{relu}$) function. A small overview over some of the key features these functions is given in \autoref{tab:activations}. Thus a network of two layers with $\tanh$ as an activation function describes the function given in \eqref{eq:network}, with $\vec{p}$  being the input to the network and $\vec{a}$ its output and the activation function being applied element-wise to the vectors. \cite[p. 2-2 - 2-12]{demuth_neural_2014}.
\begin{align}
	a_j^k &= f(w_{i,j}^k p^k_i + b_j^k) \label{eq:neuron} \\
	\vec{a} &= \vec{\tanh} (\vec{b^1} + \vec{W^1} \cdot \vec{\tanh}( \vec{b^0} + \vec{W^0} \cdot \vec{p} ) \label{eq:network}
\end{align}

\begin{table}
	\centering
	\caption{Activation functions commonly used in neural network}
	\begin{tabular}{c|c|c|c}
		\hline \hline
		Name & Domain & Range & Definition \\
		\hline
		$\tanh$ & $ (-\infty, + \infty)$ & $(-1, -1)$ & $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^-x}$ \\
		\hline
		$\sigma$ & $(-\infty, + \infty)$ & $(0, -1 )$ & $\sigma(x) = \frac{1}{1 + e^{-x}} $ \\ \hline
		$\mathrm{relu}$ & $(-\infty, + \infty)$ & $[0, +\infty)$  & $\mathrm{relu}(x) = \max(x,0)$ \\\hline \hline
	\end{tabular}\label{tab:activations}
\end{table}
\subsection{Recurrent neural networks}
It is obvious that a feed-forward network only produces an output influenced by the current activation, there can be no influence of previous activations for example in a time-series. Yet there exist many applications for which such a feature would be useful, for example the field of natural language processing \cite{ghosh_contextual_2016} or transient physical processes. This led to the development of recurrent neural networks, in which the network includes has a hidden state, which is preserved. Especially the development of the long short-term memory (lstm)  cell has had great impact on the success of recurrent neural networks due to its ability to preserve hidden states over a longer period of time \cite{hochreiter_long_1997}. An lstm layer consists of one or multiple cells, with the input being the entire or parts of the activation. Each cell passes a vector called the cell state $\vec{c}$ as well as its output $\vec{a}$ to itself in the next time step. Thus a cell has as inputs at time $t$ the cell state and output of the previous time step, $\vec{c}_{t-1}$ and $\vec{a}_{t-1}$, as well as the current activation $\vec{p}_t$. On the inside, the cell consists of a forget-gate, an input-gate and an output-gate. The forget-gate determines the influence of the cell-state, the input-gate determines the influence of the current time-step on the cell-state. The output-gate then computes the output based on the updated cell-state.
\begin{figure}
	\centering
	\def\svgwidth{0.5 \textwidth}
	\input{pics/lstm.pdf_tex}
\end{figure} 

