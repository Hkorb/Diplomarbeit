\section{Machine Learning for a Continuous Control Problem}
\subsection{Reinforcement Learning}
\subsubsection{Markov Decision Process}
The mathematical formulation on which RL is based is the Markov Decision Process (MDP). Its two main components are the agent that given a state $\vec{S}_t$ takes an action $\vec{A}_t$, and the environment, that responds to the action $\vec{A}_t$ with changing its state to $\vec{S}_{t+1}$ and giving feedback to the agent in form of a reward $R_t$. The interaction takes place at discrete time steps $t$ and the sequence of state, action and reward is referred to as the trajectory. The dynamics of the MDP are described by the function $p$ that is defined in \eqref{eq:prob_func}. It defines the probability of the state $\vec{s}'$ with reward $r$ occuring, given the state $\vec{s}$ and action $\vec{a}$. Note that the following derivations will be constricted to finite MDPs, meaning that state and action space are discrete, however the concepts all are transferable to continuous action and state space.
\begin{align}
	p(\vec{s}',r \vert \vec{s},\vec{a}) \doteq Pr(\vec{S}_t=\vec{s}', R_t=r \vert \vec{S}_{t-1} = \vec{s}, \vec{A}_{t-1} = \vec{a}) \label{eq:prob_func}
\end{align}
For a process to be a Markov Decision Process, the $p$ must only depent on $\vec{s}$ and $\vec{a}$. Therefore $\vec{s}$ must include all information necessary to determine the future behaviour of the environment. This is not limited to information currently present in the environment, when thinking of this in terms of the wind farm problem at hand, the state could include data about wind speeds at the current time but also from time steps in the past. This approach allows to model virtually any interaction as a MDP, simply by including every bit of information from the beginning of time into the state. Obviously this is not feasible and therefore a careful choice of the information in the state is necessary. \\
The goal of the learning process is to maximize the sum of the rewards in the long run. Therefore a new quantity is defined, the return $G_t$ that includes not only $R_t$ but also the rewards received in future time steps. While in many applications of RL, the process naturally comes to an end, referred to as the terminal state $\vec{S}_T$, in problems of continuous control this is not the case. Therefore the timeline is broken up into episodes. This allows for a finite computation of $G_t$. A typical formulation of $G_t$, referred to as a discounted return is given in \eqref{eq:return}. It includes a discount rate $\gamma$, that emphasizes rewards in the near future. If $\gamma = 0$, $G_t = R_t$, if $\gamma = 1$, the return is the sum of all future rewards. \cite[p. 47- 57]{sutton_reinforcement_2018}
\begin{align}
	G_t \doteq R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3}... = \sum_{t'=t}^{T}\gamma^{t'-t} R_{t'},\quad \gamma \in [0, 1] \label{eq:return}
\end{align}
Now the goal of the learning process is defined, but not what to learn. There exist three possible answers to this question: a model of the environment, a value function or a policy. Also combinations of these components is possible. In the case of continuous control, most common approaches are model-free, meaning either learning a value function, a policy or both. Therefore model-based methods will not be discussed further and the reader is referred to the book by Sutton and Barto \cite{sutton_reinforcement_2018}. \\
The policy $\pi$ is the mapping from states to actions with a set of adjustable parameters. Under the policy $\pi$ with its vector of parameters set to $\vec{\theta}$, the probability of Action $\vec{A}_t=\vec{a}$ given a state $\vec{S}_t=\vec{s}$ is denoted as $\pi_{\vec{\theta}}(\vec{a}|\vec{s})$. The value function of a state $s$ under a policy $\pi$ is denoted as $v_{\pi}(\vec{s})$ and is the expected return if the agent acts according to $\pi$, starting from state $\vec{s}$. Note that for convenience the parameters of $\pi$ were be dropped. For MDPs it can be defined as \eqref{eq:value_func}. 
\begin{align}
	v_{\pi} &\doteq \mathrm{E}_\pi \left[ G_t \vert \vec{S}_t= \vec{s} \right] =
	\mathrm{E}_\pi \left[\sum_{t'=t}^{T}\gamma^{t'-t} R_{t'} \vert \vec{S}_t=\vec{s}\right] \label{eq:value_func} \\
	&= \sum_{\vec{a}} \pi(\vec{a} \vert \vec{s}) 
	\sum_{\vec{s}'} \sum_{r} p(\vec{s}',r \vert \vec{s},\vec{a}) \left( r + \gamma v_\pi(\vec{s}') \right) \label{eq:Bellmann}
\end{align}
In the form of \eqref{eq:Bellmann}, the equation is referred to as the Bellmann equation and its unique solution is the value function $v_\pi$. Analogously, the action-value function is the expected reward of taking action $\vec{a}$ at state $\vec{s}$ under the policy $\pi$, denoted by $q_\pi(\vec{s},\vec{a})$. It is defined by \eqref{eq:action_value_func}. \cite[p. 58-59]{sutton_reinforcement_2018}
\begin{align}
	q_\pi(\vec{s},\vec{a}) \doteq \mathrm{E}_\pi \left[ G_t \vert \vec{S}_t=\vec{s}, \vec{A}_t=\vec{a} \right] =
	\mathrm{E}_\pi \left[\sum_{t'=t}^{T}\gamma^{t'-t} R_{t'} \vert \vec{S}_t=\vec{s}, \vec{A}_t=\vec{a}\right] \label{eq:action_value_func} 
\end{align}
\subsubsection{Policy Gradient Methods}
With a known value function, it is possible to construct a policy and by improving the value function, the policy can be improved. However, there exist advantages to directly improve the policy, especially for continuous state and action space. Policy gradient methods use the gradient of a perfomance measure $J(\vec{\theta})$ with respect to the parameters $\vec{\theta}$ of a policy. This gradient can be used in optimization algorithms, such as stochastic gradient descent \cite[p. 201]{sutton_reinforcement_2018} or its extensions such as Adam \cite{kingma_adam_2017}. In its basic form, SGD performs an update according to \ref{eq:sgd}. The parameter $\alpha$ is called the learning rate.
\begin{align}
	\vec{\theta}_{t+1} = \vec{\theta}_t + \alpha \nabla_{\vec{\theta}} J(\vec{\theta}) \label{eq:sgd}
\end{align}
The policy gradient methods differ now only in the performance measure. The first such algorithm proposed is the REINFORCE algorithm \cite{williams_simple_1992}. Its definition of $\nabla_{\vec{\theta}} J(\vec{\theta})$ is presented in \eqref{eq:reinforce}. It follows from the policy gradient theorem and substituting all values of $\vec{A}$ and $\vec{S}$ with the actions and states from one trajectory. Thus all the values necessary for the computation of the gradient are known. \cite[p.324-328]{sutton_reinforcement_2018}
\begin{align}
	\nabla_{\vec{\theta}} J(\vec{\theta}) 
	&= \mathrm{E}_\pi \left[ \sum_{a} \pi_{\vec{\theta}} (\vec{a}\vert \vec{S}_t) q_\pi(\vec{S}_t, \vec{a})
	\frac{\nabla_{\vec{\theta}} \pi_{\vec{\theta}}(\vec{a}\vert \vec{S}_t)}{\pi_{\vec{\theta}}(\vec{a}\vert \vec{S}_t)} \right]\label{eq:reinforce} \\
	&= \mathrm{E}_\pi \left[G_t \frac{\nabla_{\vec{\theta}} \pi_{\vec{\theta}}(\vec{A}_t\vert \vec{S}_t)}{\pi_{\vec{\theta}}(\vec{A}_t\vert \vec{S}_t)} \right] \label{eq:reinforce2}
\end{align}
While this algorithm makes a computation possible, it is inefficient. Therefore newer methods have been devised such as the Trust Region Policy Optimization (TRPO) \cite{schulman_trust_2015} and the Proximal Policy Optimization (ppo) \cite{schulman_proximal_2017}. They are closely related and both propose a surrogate performance measure that limits the size of the gradient to ensure monotonic improvement in the case of TRPO and a close approximate in the case of ppo. However, the computation of the surrogate in ppo is simpler and more efficient, which helped policy gradient methods to become one of the most used algorithms in continuous control problems. One version of the surrogate performance measure, which is also referred to as objective, is given in \eqref{eq:object_ppo}. The probability ratio $r_t$ compares the policy after the update to the policy before the update. Therefore $\pi_{\vec{\theta}}$ has to be approximated. $a_\pi(\vec{A}_t \vert \vec{S}_t)$ is an estimator of the advantage function, which is defined as $a_\pi(\vec{A}_t \vert \vec{S}_t) = q_\pi(\vec{A}_t \vert \vec{S}_t) - v_\pi(\vec{S}_t)$. This estimator also has to be found, however, this is a regular optimization problem, which can be solved via SGD or Adam.
\begin{align}
	r_t(\vec{\theta}) &\doteq \frac{\pi_{\vec{\theta}}(\vec{A}_t \vert \vec{S}_t)}{\pi_{\vec{\theta}_{old}} (\vec{A}_t \vert \vec{S}_t)} \\
	J &\doteq \mathrm{E}_\pi \left[\min \left(r_t(\vec{\theta}) a_\pi(\vec{A}_t \vert \vec{S}_t), \mathrm{clip}(r_t(\vec{\theta}), 1- \epsilon, 1+\epsilon) a_\pi(\vec{A}_t \vert \vec{S}_t) \right) \right] \label{eq:object_ppo}
\end{align}
In addition to the ratio probability clipping, other regularizations can be added to the objective function, for example an $l_2$ regularization, that adds a penalty proportional to the size of $\vec{\theta}$.
\subsection{Artificial Neural Networks}
\subsubsection{Feed-forward networks}
In the section above, a way to update parameters of the policy or value function were described, however, no description of the function itself was given. In principal any function with a set of parameters can be used, but usually, an artificial neural network (ANN) is used. ANNs are comprised of layers of neurons. More precisely a layer $k$ of $\S$ neurons takes as input a vector $\vec{p}$ of length $R$. The output of the layer is a new vector $\vec{a}^k$, wich then is the input for the next layer of the network. In a simple feed-forward layer $a_j^k$ is computed according to \eqref{eq:neuron}. The entries $w_{i,j}^k$ of the matrix $\vec{W^k}$ are called the weights of the layer and the entries $b_j^k$ of the vector $\vec{b}^k$ are called its bias. $f$ is called the activation function, which can be chosen freely, but typical choices include the hyperbolic tangent ($\mathrm{\tanh}$), the sigmoid-function ($\sigma$) or the softplus ($\mathrm{softplus}$) function. A small overview over some of the key features of these functions is given in \autoref{tab:activations}. One desirable property for activation functions is that it is differentiable at least once in the entire domain. Thus a network of two layers with $\tanh$ as an activation function describes the function given in \eqref{eq:network}, with $\vec{p}$  being the input to the network and $\vec{a}$ its output and the activation function being applied element-wise to the vectors. \cite[p. 2-2 - 2-12]{demuth_neural_2014}.
\begin{align}
	a_j^k &= f(w_{i,j}^k p^k_i + b_j^k) \label{eq:neuron} \\
	\vec{a} &= \vec{\tanh} (\vec{b^1} + \vec{W^1} \cdot \vec{\tanh}( \vec{b^0} + \vec{W^0} \cdot \vec{p} ) \label{eq:network}
\end{align}

\begin{table}
	\centering
	\caption{Activation functions commonly used in neural network}
	\begin{tabular}{c|c|c|c|c}
		\hline \hline
		Name & Domain & Range & Definition & Derivative \\
		\hline
		$\tanh$ & $ (-\infty, + \infty)$ & $(-1, -1)$ & $\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ & $1- \tanh^2(x)$\\
		\hline
		$\sigma$ & $(-\infty, + \infty)$ & $(0, -1 )$ & $\sigma(x) = (1 + e^{-x})^{-1} $ & $\sigma(x)\sigma(-x)$\\ 
		\hline 
		$\mathrm{softplus}$&  $(-\infty, + \infty)$ & $(0, +\infty)$  & $\mathrm{softplus}(x) = \ln(e^x + 1)$ & $\sigma(x)$\\
		\hline \hline
	\end{tabular}\label{tab:activations}
\end{table}
\subsubsection{Recurrent neural networks}
It is obvious that a feed-forward network only produces an output influenced by the current activation, there can be no influence of previous activations for example in a time-series. Yet there exist many applications for which such a feature would be useful, for example the field of natural language processing \cite{ghosh_contextual_2016} or transient physical processes. This led to the development of recurrent neural networks, in which the network includes has a hidden state, which is preserved. Especially the development of the long short-term memory (lstm)  cell has had great impact on the success of recurrent neural networks due to its ability to preserve hidden states over a longer period of time \cite{hochreiter_long_1997}. An lstm layer consists of one or multiple cells, with the input being the entire or parts of the activation. Each cell passes a vector called the cell state $\vec{c}$ as well as its output $\vec{a}$ to itself in the next time step. Thus a cell has as inputs at time $t$ the cell state and output of the previous time step, $\vec{c}_{t-1}$ and $\vec{a}_{t-1}$, as well as the current activation $\vec{p}_t$. On the inside, the cell consists of a forget-gate, an input-gate and an output-gate. The forget-gate determines the influence of the cell-state, the input-gate determines the influence of the current time-step on the cell-state. The output-gate then computes the output based on the updated cell-state. A schematic of the cell is given in \ref{fig:lstm}. From left to right the sigmoid layers are the forget-, input- and output-gate, whereas the $\tanh$-layer corresponds most closely to the layer of a feed-forward-network. 
\begin{figure}
	\centering
	\def\svgwidth{0.5 \textwidth}
	\input{pics/lstm.pdf_tex}
	\caption{Schematic representation of an LSTM cell, operation in boxes represent layers with trainable weights, operations in ellipses are pointwise operations.}
	\label{fig:lstm}
\end{figure} 

\subsubsection{Training of a Neural Network}
To use a neural network as policy in a policy gradient method algorithm, a way to compute the gradient of the objective $J$ has to be found. The parameters of the policy are the weights and biases of the network, therefore the partial derivatives of $J$ with respect to all weights and biases have to be found. Both described policy gradient methods express the objective gradient as a funtion $j$ of the gradient of the policy as shown in \eqref{eq:grad_f}. Under the assumption that each entry in the action vector $\vec{A}$ is a statistically indepedent random variable with the probability density function $pdf$, parameterized by $\mu_i$, the policy can be written as \eqref{eq:policy}. Thus the gradient term is \eqref{eq:logit}. \cite[p. 335]{sutton_reinforcement_2018}\\
If $\mu_i$ is the output of a $K$-layered network, it can be written as \eqref{eq:backprop1}.
Computing the gradient \eqref{eq:backprop2} can now be done efficiently via backpropagation. The name refers to the fact, that due to the chain rule, the gradient of a layer is easily expressed through the gradient of the previous layer, which allows for an efficient computation. The sensitivity $s^k_{i,j}$ determines how sensible the action is to changes of this parameter. \cite[p.11-7 - 11-13]{demuth_neural_2014}
\begin{align}
	\nabla_{\vec{\theta}} J &= j\left(\frac{\nabla_{\vec{\theta}}\pi_{\vec{\theta}}(\vec{A}|\vec{S})} {\pi_{\vec{\theta}}(\vec{A}|\vec{S})}\right) \label{eq:grad_f}\\
	\pi_{\vec{\theta}}(\vec{A}|\vec{S}) &= \prod_i pdf(A_i, \mu_i(\vec{S},\vec{\theta})) \label{eq:policy}\\	\frac{\nabla_{\vec{\theta}}\pi_{\vec{\theta}}(\vec{A}|\vec{S})} {\pi_{\vec{\theta}}(\vec{A}|\vec{S})} &= \prod_i \frac{\partial pdf(A_i, \mu_i(\vec{S},\vec{\theta}))}{\partial \mu_i(\vec{S}, \vec{\theta})} \frac{\nabla_{\vec{\theta}} \mu_i(\vec{S},\vec{\theta})}{pdf(A_i, \mu_i(\vec{S}, \vec{\theta}))} \label{eq:logit}\\
	\mu_i(\vec{S}, \vec{\theta}) &= f^K ( b^K_i+w^K_{i,j} f^{K-1} (...f^{0}(b^0_l + w^0_{l,m}S_m)...)) \label{eq:backprop1} \\
	\nabla_{\vec{\theta}} \mu_i |_{\vec{S}} &= \left[\left. \frac{\partial \mu_i}{\partial b^k_j} \right|_{\vec{S}}, 
	\left.\frac{\partial \mu_i}{\partial w^k_{j,l}}\right|_{\vec{S}} \right]^T \label{eq:backprop2}\\
	x^k_i &= b^{k}_i + w^{k}_{i,j}p^k_j, \quad p^{k+1}_i = f^k(x^k_i), \quad p^0_i = S_i \\
	s^K_{i,j} &= \left.\frac{\partial f^K(x)}{\partial x}\right|_{x^K_i} \delta_{i,j}, \quad \left.s^{k}_{i,j} = s^{k+1}_{i,l} w^k_{l,j} \frac{\partial f^k(x)}{\partial x}\right|_{x^k_i} \\
	\left. \frac{\partial \mu_i}{\partial b^k_j}\right|_{\vec{S}} &= s^k_{i,j}, \quad \left. \frac{\partial \mu_i}{\partial w^k_{j,l}} \right|_{\vec{S}} = s^k_{i,j} p^k_l
\end{align}