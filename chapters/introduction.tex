% !TEX root = ../Diploma.tex
While human made climate change only begins to effect the countries of Europe and North America, the predictions for the near and far future are devastating \cite{hoegh-guldberg_impacts_2019}. One of the main drivers of climate change is the increase in concentration of greenhouse gases in the atmosphere. The production of electricity with fossil fuels accounts for up to a third of greenhouse gas emissions in major industrial countries like the United States \cite{hockstad_inventory_2018} and Germany \cite{ortl_entwicklung_2020}. To reduce the emission of greenhouse gases the energy sector is shifting to renewable sources of energy such as wind and solar \cite{international_energy_agency_global_2020}.\\
To reduce investment and maintenance costs, wind turbines are often arranged in wind farms of multiple, sometimes hundreds of turbines. However, this also reduces the overall efficiency of the turbines due to wake losses \cite{nilsson_large-eddy_2015}. The simplest way to reduce the wake losses would be to increase the spacing between turbines. However, this reduces the aforementioned benefits of wind farms and can not be done after the farm has been built. Another approach is to control the turbines in a way, that reduces the deficits due to wakes. This approach has the advantage of not placing restrictions on the layout of the park and can applied after the park has been built. Wake interaction can be reduced in two ways. The wake is either steered away from the downstream turbines, usually by changing the yaw of the turbine or the wake deficit is reduced by curtailing the upstream turbines. Curtailment was first studied by Steinbuch et al. \cite{steinbuch_optimal_1988}. Multiple studies since then have tested this strategy and a recent survey of them found that a static curtailment does not significantly increase the overall power production, when tested with high fidelity simulations \cite{kheirabadi_quantitative_2019}. However, the exploration of dynamic approaches has also begun. Goit and Munters used receding horizon optimal control to prove that an increase of $16$ percent is possible \cite{goit_optimal_2015}. Based on these results, Munters and Meyers proposed a sinusoidal variation of $C_P$ and also found increased production \cite{munters_towards_2018}. This was also confirmed in wind tunnel experiments and further simulations by Frederik et al. \cite{frederik_periodic_2020}. Based on these results, Frederik et al. used an oscillating pitch angle to induce a moment on the wake to steer the wake in a helical motion, also recording increases in power production in a setup of two turbines by $7.5$ percent \cite{frederik_helix_2020}. \\
The interactions of wake and turbines in a wind farm can be viewed as a non-linear system. Methods of machine learning has proven to be a very useful tool to tackle non-linear systems and non-linear control. One method suitable for non-linear control is reinforcement learning (RL). The field of RL emerged as a combination  of dynamic programming and the theory of trial-and-error-learning in psychology. They were first combined into the modern field of RL in the 1970s and 1980s through pioneering work by Klopf, Barto and Sutton \cite[p. 20-21]{sutton_reinforcement_2018}. RL has been used in a vast range of applications, such as playing Go and Chess \cite{silver_general_2018} or controlling a Mars lander  \cite{gaudet_deep_2020}. One group of methods from the field of RL is policy-gradient methods. In active flow control the first application of policy-gradient methods was by Rabault to reduce drag in a von Kármán vortex street \cite{rabault_artificial_2019}. Furthermore, it was used by Belus et al. to stabilize a thin fluid film \cite{belus_exploiting_2019}.  \\
A critical point when combining fluid simulations with RL is the high computational cost of a single time step of the fluid simulation and the large number of time steps required for RL. The lattice Boltzmann method has shown great potential to lower the computational cost of fluid simulations \cite{lohner_towards_2019}. In contrast to fluid solvers based on the Navier-Stokes equations it can easily take advantage of the highly parallel hardware such as graphical processing units \cite{kutscher_multiscale_2019}. It is also suitable for highly turbulent flows \cite{gehrke_scrutinizing_2017}. Asmuth et al. recently applied it to large eddy simulations of wind farms for the first time \cite{asmuth_actuator_2019}.\\
In this work, reinforcement learning will be applied to maximize power production in an LBM-LES simulation of a small wind park. In the first chapter, the theoretical foundations for wind turbine control and simulation, as well as RL and LBM are laid and a more detailed explanation of already existing control strategies will be given. In the second chapter, the implementation and setup of the simulations is explained. In the following chapter the results of the simulations will be presented and examined. Finally a conclusion and outlook into future developments is given.